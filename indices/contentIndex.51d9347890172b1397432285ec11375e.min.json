{"/":{"title":"Ben's Math Notes","content":"\nMy name is Ben. I am currently interested in algebraic geometry and algebraic number theory. This page will hopefully include everything that I learn during my master's degree, including my reading over the summer to prepare for it. \n\nTable of Contents:\n- [Complex Analysis](notes/Complex%20Analysis/Complex%20Analysis.md)\n- [Representation Theory](notes/Representation%20Theory/Representation%20Theory.md)\n- [Algebraic Geometry](notes/Algebraic%20Geometry/Algebraic%20Geometry.md)\n- [Commutative Algebra](notes/Commutative%20Algebra/Commutative%20Algebra.md)\n\n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":[]},"/notes/Algebraic-Geometry/Algebraic-Geometry":{"title":"Algebraic Geometry","content":"Table of Contents\n- [Algebraic Sets](notes/Algebraic%20Geometry/Algebraic%20Sets.md)\n- [The Zariski Topology](notes/Algebraic%20Geometry/The%20Zariski%20Topology.md)\n- ","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["algebraic-geometry"]},"/notes/Algebraic-Geometry/Algebraic-Sets":{"title":"Affine Varieties","content":"Throughout this note fix $k$ to be an algebraically closed field, this could be $\\mathbb C, \\overline{\\mathbb Q}, \\overline{\\mathbb{F}_p}$ etc. \n\n**Definition (Affine $n$-space).** *Affine $n$-space* over $k$, denoted $\\mathbb A\\_k^n$ or just $\\mathbb A^n$, is the space of all $n$-tuples of elements of $k$. An element $P \\in \\mathbb A^n$ we be called a *point*, and if $P = (a\\_1, \\ldots , a\\_n)$ with $a\\_i \\in k$, then the $a\\_i$ will be called the *coordinates* of $P$. \n\nWe don't just denote affine $n$-space by $k^n$ because soon we will endow $\\mathbb A^n$ with more structure, namely a topology. \n\nLet $A = k[x\\_1, \\ldots , x\\_n]$ be the polynomial ring in $n$ variables over $k$. We will interpret the elements of $A$ as functions from $\\mathbb A^n$ to $k$ by defining $f(P) = f(a\\_1, \\ldots , a\\_n)$ where $f \\in A$ and $P \\in \\mathbb A^n$. \n\n**Definition (Zero Set).** If $f \\in A$ is a polynomial, we can talk about the *zeros* of $f$, namely the set \n$$\nZ(f) = \\lbrace P \\in \\mathbb A^n \\mid f(P) = 0 \\rbrace.\n$$More generally, if $T$ is any subset of $A$, we define the *zero set* of $T$ to be the common zeros of all the elements of $T$, namely \n$$\nZ(T) = \\lbrace P \\in \\mathbb A^n \\mid f(P) = 0 \\text{ for all } f \\in T \\rbrace. \n$$\n\nIt is important to note that if $\\mathfrak a = \\langle T \\rangle$ is the ideal generated by $T$, then $Z(\\mathfrak a) = Z(T)$. Furthermore, since $A$ is a Noetherian ring by the [Hilbert Basis Theorem](notes/Commutative%20Algebra/Hilbert%20Basis%20Theorem.md), $\\mathfrak a$ has a finite set of generators $f_1, \\ldots , f_r$. To see the equality of the two sets first let $P \\in Z(T)$. Then for any $g = \\sum\\_{i=1}^r a\\_i f\\_i$ we have \n$$\ng(P) = \\sum\\_{i=1}^r a\\_if\\_i(P) = 0 \n$$\nso $P \\in Z(\\mathfrak a)$. Now if $P \\in Z(\\mathfrak a)$, since each $f \\in T$ is also in $\\mathfrak a$, we must have $f(P) = 0$ for all $f \\in T$. Thus the sets are equal. So we will only refer to $Z(\\mathfrak a)$ where $\\mathfrak a \\subset A$ is an ideal, and it can be thought of as the common set of zeros of the generators of $\\mathfrak a$. \n\n**Definition (Closed Algebraic Set).** A subset $Y$ of $\\mathbb A^n$ is a *closed algebraic set* if there exists a subset $T \\subseteq A$ such that $Y = Z(T)$. \n\n**Examples.**\n1. The polynomial ring $k\\[x\\]$ is a PID, i.e., every ideal is generated by one element. So if $\\mathfrak a$ is an ideal in $k\\[x\\]$, then $\\mathfrak a = (f(x))$. Because polynomials in one variable have only finitely many zeros, the closed algebraic sets of $\\mathbb A\\^1$ are just the finite subsets of $\\mathbb A\\^1$. \n2. The traditional conic sections are closed algebraic sets in $\\mathbb A^2$, i.e., polynomials of the form $ax^2 + by^2 + cxy + dx + ey + f$. For instance the zeros of $y-x^2$ form a parabola and $xy-1$ gives a hyperbola. \n3. The *Clebsch diagonal cubic* is a surface in $\\mathbb A\\_{\\mathbb C}^3$ given by the equation $$x^3 + y^3 + z^3 + 1 = (x+y+z+1)^3.$$\nNow we will explore a couple of properties of closed algebraic sets. \n\n**Proposition.** If $\\mathfrak b \\subset \\mathfrak a$, then, $Z(\\mathfrak a) \\subset Z(\\mathfrak b)$.\n\n*Proof.* If $P \\in Z(\\mathfrak a)$ and $f \\in \\mathfrak b$, then since $f \\in \\mathfrak a$ as well, $f(P) = 0$, so $P \\in Z(\\mathfrak b)$.\n\n**Proposition.** If $\\mathfrak a \\subset k\\[x\\_1, \\ldots, x\\_n \\]$ is an ideal, then $Z(\\mathfrak a) = Z(\\sqrt{\\mathfrak a})$. \n\n*Proof.* Let $n \\in \\N$. Then $\\mathfrak a^n = \\lbrace \\sum f\\_1 \\cdots f\\_n \\mid f\\_i \\in \\mathfrak a \\rbrace \\subseteq \\mathfrak a$ .So by the above proposition $Z(\\mathfrak a) \\subseteq Z(\\mathfrak a^n)$. Now let $P \\in Z(\\mathfrak a^n)$. Since $f^n \\in \\mathfrak a^n$ whenever $f \\in \\mathfrak a$, we have that $0 = f^n(P) = (f(P))^n$. Since $k$ is an integral domain this implies that $f(P) = 0$ and $P \\in Z(\\mathfrak a)$. Thus putting this together for all $n \\in \\N$ gives $Z(\\mathfrak a) = Z(\\mathfrak a^n)$. ","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["algebraic-geometry"]},"/notes/Algebraic-Geometry/The-Zariski-Topology":{"title":"The Zariski Topology","content":"We start by proving some properties of closed algebraic sets. \n\n**Proposition.** The union of two [Algebraic Sets](notes/Algebraic%20Geometry/Algebraic%20Sets.md) is algebraic, $Z(\\mathfrak a) \\cup Z(\\mathfrak b) = Z(\\mathfrak a \\cap \\mathfrak b)$, the intersection of any family of algebraic sets is algebraic, $\\bigcap\\_{i \\in I} Z(\\mathfrak a_i) = Z\\left(\\sum\\_{i \\in I} \\mathfrak a\\_i \\right)$. The empty set and the whole space are also algebraic sets. \n\n*Proof.* If $Y\\_1 = Z(\\mathfrak a\\_1)$ and $Y\\_2 = Z(\\mathfrak a\\_2)$, then $Y\\_1 \\cup Y\\_2 = Z(\\mathfrak a\\_1 \\mathfrak a\\_2)$. Indeed, if $P \\in Y\\_1 \\cup Y\\_2$, then either $P \\in Y\\_1$ or $P\\in Y\\_2$ so $P$ is a zero of every polynomial in $\\mathfrak a\\_1 \\mathfrak a\\_2 = \\lbrace \\sum afg \\mid f \\in \\mathfrak a\\_1, g \\in \\mathfrak a\\_2, a \\in k \\rbrace$. Conversely, if $P \\in Z(\\mathfrak a\\_1 \\mathfrak a\\_2)$ and $P \\notin Y\\_1$, say, then there is an element $f \\in \\mathfrak a\\_1$ such that $f(P) \\neq 0$. Now, for any $g \\in \\mathfrak a\\_2$, $(fg)(P) = 0$ implies that $P \\in Z(\\mathfrak a\\_2)$ and $Y\\_1 \\cup Y\\_2 = Z(\\mathfrak a\\_1 \\mathfrak a\\_2)$. \n\nNext, recall that $\\mathfrak a + \\mathfrak b = \\lbrace f + g \\mid f \\in \\mathfrak a, g \\in \\mathfrak b \\rbrace$ where $\\mathfrak a, \\mathfrak b$ are ideals of $k\\[x\\_1, \\ldots , x\\_n \\]$. Then $Z(\\mathfrak a) \\cap Z(\\mathfrak b) = Z(\\mathfrak a + \\mathfrak b)$. We can see this by first taking $P \\in Z(\\mathfrak a + \\mathfrak b)$, then $(f + 0)(P) = f(P) = 0$ and $(0 + g)(P) = g(P) = 0$, so $P \\in Z(\\mathfrak a) \\cap Z(\\mathfrak b)$. Conversely, if $P \\in Z(\\mathfrak a) \\cap Z(\\mathfrak b)$, then if $f + g \\in \\mathfrak a + \\mathfrak b$, $(f + g)(P) = f(P) + g(P) = 0$. So the sets are equal.\n\nFinally, $\\emptyset = Z(1)$ and $\\mathbb A^n = Z(0)$. \n\nThe above proposition tells us that we actually get a topology on $\\mathbb A^n$ by taking the $Z(\\mathfrak a)$ to be the closed sets.\n\n**Definition (Zariski Topology).** We define the *Zariski topology* on $\\mathbb A^n$ by taking the open subsets to be the complements of the [Algebraic Sets](notes/Algebraic%20Geometry/Algebraic%20Sets.md). This is a topology, because according to the proposition, the intersection of two open sets is open, and the union of any family of open sets is open. Furthermore, the empty set and the whole space are both open. \n\n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["algebraic-geometry"]},"/notes/Commutative-Algebra/Commutative-Algebra":{"title":"Commutative Algebra","content":"\nTable of Contents\n- [Hilbert Basis Theorem](notes/Commutative%20Algebra/Hilbert%20Basis%20Theorem.md)","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["commutative-algebra"]},"/notes/Commutative-Algebra/Hilbert-Basis-Theorem":{"title":"Hilbert Basis Theorem","content":"In this note we prove that a polynomial ring over a Noetherian ring is Noetherian. The idea of the proof is to take an ideal $I$ of $R[x]$ and create an ideal in $R$ that consists of all the leading coefficients of polynomials in $I$. Then we can use that fact that this ideal in $R$ is finitely generated to prove that $I$ is finitely generated. \n\n**Theorem (Hilbert Basis Theorem).** Let $R$ be a Noetherian ring. Then the polynomial ring $R[x]$ is Noetherian as well. \n\n*Proof.* Let $I \\subseteq R[x]$ be an ideal. We need to show that $I$ is finitely generated. For $i \\in \\N$, set \n\n$$\nJ_i = \\lbrace a_i \\in R \\mid \\text{there exists $a_0, \\ldots , a_{i-1} \\in R$ such that } \\sum_{j=0}^i a_jx^j \\in I \\rbrace.\n$$Clearly $J_i \\subseteq R$ is an ideal. These ideals consist of elements of $R$ that can be the leading coefficient for a polynomial in $I$. Let $a_i \\in J_i$ with $f = \\sum_{j=0}^i a_jx^j \\in I$. Then $xf = \\sum\\_{j=0}^i a\\_j x^{j+1} \\in I$, so $a_{i} \\in a_{j+1}$ as well. Thus we have an ascending chain of ideals \n$$\nJ_1 \\subseteq J_2 \\subseteq J_2 \\subseteq \\cdots.\n$$However, since these are ideals of $R$ and $R$ is Noetherian, there exists an $n \\in \\N$ such that for $i \\geq n$ we have $J_i = J_n$. And since each $J_i$ is finitely generated we have \n$$\nJ\\_i = \\left(a\\_{i, 1}, \\ldots , a\\_{i, m\\_i} \\right)\n$$for $i \\leq n$, and \n$$\nJ_i = J_n = \\left(a\\_{n, 1}, \\ldots , a\\_{n, m\\_n}\\right)\n$$for $i \u003e n$. By the definition of $J_i$, there exist polynomials $f_{i, j} \\in I$ of degree at most $i$ whose $i$th coefficient is $a\\_{i,j}$. Set \n$$\nI' = \\left(f\\_{i, j} \\mid i =0, \\ldots , n, j =1, \\ldots, m\\_i \\right) \\subset I.\n$$We claim that $I = I'$. To prove this, consider a polynomial $f = \\sum\\_{i=0}^d b\\_ix^i \\in I$ with $\\deg(f) = d$. We will us induction on $d$. First consider the case $d \\leq n$. Since $b_d \\in J_d$ we can write \n$$\nb\\_d = \\sum\\_{j=1}^{m\\_d} r\\_ja\\_{d, j}\n$$with $r\\_j \\in R$. Then \n$$\n\\widetilde f = f - \\sum\\_{j=1}^{m\\_d}r\\_jf\\_{d, j} \n$$lies in $I$ and has degree less than $d$, so by induction $\\widetilde f \\in I'$. This implies that $f \\in I'$ since each $f_{d, j} \\in I'$ by definition. Now assume $d \u003e n$. Then we can write $b\\_d = \\sum\\_{j=1}^{m\\_n} r\\_j a\\_{n, j}$ with $r\\_j \\in R$. So \n$$\n\\widetilde f = f - \\sum\\_{j=1}^{m\\_n} r\\_j x^{d-n}f\\_{n, j}\n$$lies in $I$ and has degree less than $d$, so by induction $\\widetilde f \\in I'$. Again we conclude that $f \\in I'$. So indeed $I = I'$ is a finitely generated ideal. ","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["commutative-algebra"]},"/notes/Complex-Analysis/Cauchy-Riemann-Equations":{"title":"Cauchy-Riemann Equations","content":"In this note we will make a connection between the differentiability of complex functions and the differentiability of real functions. We first consider the following example.\n\n**Example.** Let $f(z) = \\overline z$, where $\\overline z = \\overline{x + iy} = x-iy$ is complex conjugation. Then $f$ is not [holomorphic](notes/Complex%20Analysis/Holomorphic%20Functions.md). Indeed, we have \n$$\n\\frac{f(z_0 + h) - f(z_0)}{h} = \\frac{\\overline h}{h}.\n$$If we consider $h$ real, then $\\overline h = h$, and the above quotient is 1. However, if we let $h$ be purely imaginary, $h = iy$, then the above quotient is -1. Thus the limit does not exist and $f$ is not holomorphic. However, as a real function $f$ is \n$$\nF \\colon (x, y) \\mapsto (x, -y)\n$$\nwhich is differentiable in real variables with Jacobian matrix equal to \n$$\n\\begin{pmatrix}1 \u0026 0\\\\ 0 \u0026 -1 \\end{pmatrix}. \n$$Thus if a function is differentiable as a map $\\R^2 \\to \\R^2$ then it is not necessarily differentiable as a map $\\mathbb C \\to \\mathbb C$. \n\nNow, consider the difference quotient for a complex function when $h$ is real, $h = h_1 + ih_2$ with $h_2 = 0$, i.e., \n$$\nf'(z\\_0) = \\lim\\_{h_1 \\to 0} \\frac{f(x\\_0 + h\\_1, y\\_0) - f(x\\_0, y\\_0)}{h_1}.\n$$But notice that this is exactly the partial derivative of $f$ with respect to $x$ if we write $z = x + iy$ and $f(z) = f(x, y)$. Thus \n$$\nf'(z\\_0) = \\frac{\\partial f}{\\partial x}(z\\_0).\n$$Next, consider what happens if $h = h_1 + ih_2$ with $h_1 = 0$. Then we have \n$$\nf'(z\\_0) = \\lim\\_{h\\_2 \\to 0} \\frac{f(x\\_0, y\\_0 + h\\_2) - f(x\\_0, y\\_0)}{ih\\_2} = \\frac{1}{i}\\frac{\\partial f}{\\partial y}. \n$$\nThis yields the following equation \n$$\n\\frac{\\partial f}{\\partial x} = \\frac{1}{i}\\frac{\\partial f}{\\partial y}\n$$Writing $f = u + iv$ where $u, v \\colon \\R^2 \\to \\R$ we have \n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial u}{\\partial x} + i \\frac{\\partial v}{\\partial x}, \\hspace{0.2cm}\\frac{1}{i}\\frac{\\partial f}{\\partial y} = \\frac{1}{i}\\frac{\\partial u}{\\partial y} + \\frac{\\partial v}{\\partial y}. \n$$\nNow this gives us \n$$\n\\frac{\\partial u}{\\partial x} + i \\frac{\\partial v}{\\partial x} = \\frac{1}{i}\\frac{\\partial u}{\\partial y} + \\frac{\\partial v}{\\partial y}\n$$and separating the real and imaginary parts gives two equations \n$$\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\hspace{.2cm} \\frac{\\partial v}{\\partial x} = -\\frac{\\partial u}{\\partial y}.\n$$\nThese are called the *Cauchy-Riemann equations* and every holomorphic function satisfies them. And not only that, we will later show the converse, any complex function satisfying the Cauchy-Riemann equations is holomorphic. This provides a bridge between real differentiability and complex differentiability. \n\nNow we introduce two operators that will help us prove some nice relations between the complex derivative at a point and derivatives of real functions. Let \n$$\n\\frac{\\partial}{\\partial z} = \\frac{1}{2}\\left(\\frac{\\partial}{\\partial x} + \\frac{1}{i}\\frac{\\partial}{\\partial y} \\right) \\text{ and } \\frac{\\partial}{\\partial \\overline{z}} = \\frac{1}{2} \\left(\\frac{\\partial}{\\partial x} - \\frac{1}{i} \\frac{\\partial}{\\partial y} \\right).\n$$\nThese are defined with a $\\frac{1}{2}$ to make the following proposition nicer. \n\n\n**Proposition.** If $f$ is holomorphic at $z_0$, then \n$$\n\\frac{\\partial f}{\\partial \\overline z}(z_0) = 0 \\text{ and } f'(z_0) = \\frac{\\partial f}{\\partial z}(z_0) = 2 \\frac{\\partial u}{\\partial z}(z_0).\n$$Also, if we write $F(x, y) = f(z)$, then $F$ is differentiable in the sense of real variables, and \n$$\n\\det J_F(x_0, y_0) = \\left\\vert f'(z_0) \\right\\vert^2. \n$$\n\n*Proof.* First, using the Cauchy-Riemann equations we have \n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial \\overline z}(z\\_0) =\u0026 \\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x} - \\frac{1}{i} \\frac{\\partial f}{\\partial y} \\right)\\\\\n=\u0026\\; \\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x} + i \\frac{\\partial v}{\\partial x} - \\frac{1}{i} \\frac{\\partial u}{\\partial y} - \\frac{\\partial v}{\\partial y} \\right)\\\\\n=\u0026 \\left(\\frac{\\partial v}{\\partial y} + i \\frac{\\partial v}{\\partial x} + \\frac{1}{i} \\frac{\\partial v}{\\partial x} - \\frac{\\partial v}{\\partial y} \\right) = 0. \n\\end{align*}\n$$Moreover, we have already showed that $f'(z_0) = \\frac{\\partial f}{\\partial x}(z\\_0) = \\frac{1}{i}\\frac{\\partial f}{\\partial y}(z\\_0)$ and thus \n$$\nf'(z\\_0) = \\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x}(z\\_0) + \\frac{1}{i} \\frac{\\partial f}{\\partial y}(z\\_0) \\right) = \\frac{\\partial f}{\\partial z}. \n$$\nThen by the Cauchy-Riemann equations we have \n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial z} =\u0026 \\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x} + \\frac{1}{i} \\frac{\\partial f}{\\partial y} \\right)\\\\\n=\u0026 \\left(\\left(\\frac{\\partial u}{\\partial x} + i \\frac{\\partial v}{\\partial x} \\right) + \\frac{1}{i}\\left(\\frac{\\partial u}{\\partial y} + i \\frac{\\partial v}{\\partial y} \\right) \\right)\\\\\n=\u0026 \\frac{1}{2} \\left(\\frac{\\partial u}{\\partial x} - i \\frac{\\partial u}{\\partial y} + \\frac{1}{i} \\frac{\\partial u}{\\partial y} + \\frac{\\partial u}{\\partial x} \\right)\\\\\n=\u0026 \\frac{1}{2} \\left(2 \\frac{\\partial u}{\\partial x} + \\frac{2}{i} \\frac{\\partial u}{\\partial y} \\right) = 2\\frac{\\partial u}{\\partial z}. \n\\end{align*}\n$$To prove that $F$ is differentiable it suffices to observe that if $H = (h\\_1, h\\_2)$ and $h = h\\_1 + i h\\_2$, then the Cauchy-Riemann equations imply \n$$\nJ_F(x\\_0, y\\_0)(H) = \\left(\\frac{\\partial u}{\\partial x} - i \\frac{\\partial u}{\\partial y} \\right)(h\\_1 + ih\\_2) = f'(z\\_0)h.\n$$\nThis is because we have \n$$\n\\begin{align*}\nJ_F(x\\_0, y\\_0)(H) =\u0026 \\begin{pmatrix}\n\\partial u/\\partial x \u0026 \\partial u/\\partial y\\\\ \\partial v/\\partial x \u0026 \\partial v/\\partial y\n\\end{pmatrix}\\begin{pmatrix}\nh\\_1 \\\\ h\\_2\n\\end{pmatrix}\\\\\n=\u0026 \\begin{pmatrix}\n\\partial u/\\partial x \u0026 \\partial u/\\partial y\\\\ -\\partial u/\\partial y \u0026 \\partial u/\\partial x\n\\end{pmatrix}\\begin{pmatrix}\nh\\_1 \\\\ h\\_2\n\\end{pmatrix}\\\\\n=\u0026 \\left(\\frac{\\partial u}{\\partial x} - i \\frac{\\partial u}{\\partial y} \\right)(h\\_1 + i h\\_2)\n\\end{align*}\n$$by identifying matrices of the form $\\begin{pmatrix} a \u0026 -b \\\\ b \u0026 a\\end{pmatrix}$ with $\\mathbb C$ and $\\begin{pmatrix} h\\_1 \\\\  h\\_2 \\end{pmatrix}$ with $h\\_1 + i h\\_2$. Then \n$$\n\\frac{\\partial u}{\\partial x} - i \\frac{\\partial u}{\\partial y} = \\frac{\\partial u}{\\partial x} + \\frac{1}{i} \\frac{\\partial u}{\\partial y} = 2 \\frac{\\partial u}{\\partial z} = f'(z\\_0). \n$$To get the final result we have \n$$\n\\det J\\_F(x\\_0, y\\_0) = \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial y} - \\frac{\\partial v}{\\partial x} \\frac{\\partial u}{\\partial y} = \\left(\\frac{\\partial u}{\\partial x}\\right)\\^2\n \\+ \\left(\\frac{\\partial u}{\\partial y} \\right)\\^2 = \\left \\vert 2 \\frac{\\partial u}{\\partial z} \\right \\vert^2 = \\left \\vert f'(z\\_0) \\right\\vert$$ \nThis result tells us that if a function is holomorphic then it is differentiable as a function of real variables, but as we saw in the first example, the converse is not true. \n\nNow we will see that any holomorphic function that satisfies the Cauchy-Riemann equations is actually holomorphic. \n\n**Theorem.** Suppose $f = u + iv$ is a complex-valued function defined on an open set $\\Omega$. If $u$ and $v$ are continuously differentiable and satisfy the Cauchy-Riemann equations on $\\Omega$, then $f$ is holomorphic on $\\Omega$ and $f'(z) = \\frac{\\partial f}{\\partial z}$. \n\n*Proof.* We can write \n$$\nu(x + h\\_1, y + h\\_2) - u(x, y) = \\frac{\\partial u}{\\partial x}h\\_1 + \\frac{\\partial u}{\\partial y}h\\_2 + |h|\\psi\\_1(h)\n$$and \n$$\nv(x + h\\_1, y + h\\_2) - v(x, y) = \\frac{\\partial v}{\\partial x}h\\_1 + \\frac{\\partial v}{\\partial y}h\\_2 + |h|\\psi\\_2(h)\n$$where $\\psi\\_j(h) \\to 0$ as $|h| \\to 0$ and $h = h\\_1 + ih\\_2$. Using the Cauchy-Riemann equations we find that \n$$\n\\begin{align*}\nf(z + h) - f(z) =\u0026 u(x + h\\_1, y + h\\_2) + iv(x + h\\_1, y + h\\_2) - u(x, y) - v(x, y)\\\\\n=\u0026 \\frac{\\partial u}{\\partial x}h\\_1 + \\frac{\\partial u}{\\partial y}h\\_2 + i\\frac{\\partial v}{\\partial x} + i \\frac{\\partial v}{\\partial y} + |h|\\left(\\psi\\_1(h) + \\psi\\_2(h) \\right)\\\\\n=\u0026 \\frac{\\partial u}{\\partial x}h\\_1 + \\frac{\\partial u}{\\partial y}h\\_2 - i \\frac{\\partial u}{\\partial y}h\\_1 + i \\frac{\\partial u}{\\partial x}h\\_2 + |h|\\psi(h)\\\\\n=\u0026 \\left(\\frac{\\partial u}{\\partial x} - i \\frac{\\partial u}{\\partial y} \\right)(h\\_1 + ih\\_2) + |h|\\psi(h)\\\\\n=\u0026 \\left(\\frac{\\partial u}{\\partial x} + \\frac{1}{i} \\frac{\\partial u}{\\partial y} \\right)(h\\_1 + ih\\_2) + |h|\\psi(h).\n\\end{align*}\n$$Therefore, $f$ is holomorphic and \n$$\nf'(z) = 2\\frac{\\partial u}{\\partial z} = \\frac{\\partial f}{\\partial z}. \\hspace{.2cm} \n$$\nThis theorem shows us that a function is holomorphic if and only if it satisfies the Cauchy-Riemann equations. ","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["complex-analysis"]},"/notes/Complex-Analysis/Complex-Analysis":{"title":"Complex Analysis","content":"Table of Contents\n- [Holomorphic Functions](notes/Complex%20Analysis/Holomorphic%20Functions.md)\n- [Cauchy-Riemann Equations](notes/Complex%20Analysis/Cauchy-Riemann%20Equations.md)\n- [Power Series](notes/Complex%20Analysis/Power%20Series.md) \n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["complex-analysis"]},"/notes/Complex-Analysis/Holomorphic-Functions":{"title":"Holomorphic Functions","content":"Holomorphic functions are the complex analog of differentiable real functions, but have many nicer properties.\n\n**Definition (Holomorphic Function).** Let $\\Omega$ be an open set in $\\mathbb C$ and $f \\colon \\Omega \\to \\mathbb C$. The function $f$ is *holomorphic at the point* $z_0 \\in \\mathbb C$ if the quotient \n$$\n\\frac{f(z_0 + h) - f(z_0)}{h}\n$$\nconverges to a limit when $h \\to 0$. When this limit exists it is denoted $f'(z_0)$ and is called the derivative of $f$ at $z_0$: \n$$\nf'(z_0) = \\lim_{h \\to 0} \\frac{f(z_0+h) - f(z_0)}{h}.\n$$We can also say that a function $f$ is holomorphic at $z_0 \\in \\Omega$ if and only if there exists $a \\in \\mathbb C$ such that \n$$\nf(z_0 + h) - f(z_0) - ah = h \\psi(h)\n$$where $\\psi$ is a function defined for small $h$ and $\\lim_{h \\to 0} \\psi(h) = 0$. We have $a = f'(z_0)$, if it exists. \n\nNotice that this is very similar to the case in one variable. And we even have the following proposition which is proved in the exact same way as in one variable. \n\n**Proposition.** If $f$ and $g$ are holomorphic in $\\Omega$ then \n1. $f + g$ is holomorphic in $\\Omega$ and $(f + g)' = f' + g'$.\n2. $fg$ is holomorphic in $\\Omega$ and $(fg)' = f'g + fg'$.\n3. If $g(z_0) \\neq 0$ then $f/g$ is holomorphic at $z_0$ and $(f/g)' = \\frac{f'g - fg'}{g^2}$. \nMoreover, if $f \\colon \\Omega \\to U$ and $g \\colon \\Omega \\to \\mathbb C$ are holomorphic, then the chain rule holds:\n$$\n(g\\circ f)' = g'(f(z))f'(z).\n$$\n\n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["complex-analysis"]},"/notes/Complex-Analysis/Power-Series":{"title":"Power Series","content":"The goal of this note is to show that every [holomorphic](notes/Complex%20Analysis/Holomorphic%20Functions.md) function is analytic and vice versa. \n\n**Definition (Power Series).** A *power series* is an expression of the form $\\sum_{n=0}^\\infty a_nz^n$ where $a_n \\in \\mathbb C$. \n\nThe first natural question to ask is when does a power series converge? Hadamard's formula will give us the answer.  \n\n**Theorem.** Given a power series $\\sum_{n=0}^\\infty a_nz^n$ there exists $0 \\leq R \\leq \\infty$ such that \n1. if $|z| \u003c R$ the series converges absolutely. The disk $|z| \u003c R$ is called the *disc of convergence*.\n2. if $|z| \u003e R$ the series diverges. \nWe call $R$ the *radius of convergence*. Moreover, if we use the convention that $1/0 = \\infty$ and $1/\\infty = 0$, then $R$ is given by Hadamard's formula \n$$\n\\frac{1}{R} = \\limsup |a_n|^{1/n}. \n$$\nNote that this theorem does not say anything about the behavior at $|z| = R$. There are several different things that can happen there. \n\n*Proof.* Let $L = 1/R$ where $R$ is defined by the formula in the statement of the theorem, and suppose that $L \\neq 0, \\infty$. If $|z| \u003c R$, choose $\\epsilon \u003e 0$ such that \n$$\n(L + \\epsilon)|z| = r \u003c 1.\n$$By the definition of $L$, we have that $|a_n|^{1/n} \\leq L + \\epsilon$ for all large $n$, therefore\n$$\n|a_n||z|^n \\leq \\left((L+ \\epsilon)|z| \\right)^n = r^n.\n$$Comparing our series with the geometric series $\\sum_{n=0}^\\infty r^n$ shows that $\\sum_{n=0}^\\infty a_nz^n$ converges.\n\nNow, if $|z| \u003e R$, the we can choose $\\epsilon$ such that $(L - \\epsilon)|z| = r \u003e 1$. Then $|a_n|^{1/n} \\geq L - \\epsilon$ for large $n$ so \n$$\n|a_n||z|^n \\geq \\left((L - \\epsilon)|z| \\right)^n = r^n \u003e 1.\n$$\nand since $\\sum_{n=0}^\\infty r^n$ diverges, so will $\\sum_{n=0}^\\infty a_nz^n$. \n\nNow we can see that a power series is holomorphic. The idea is to break up the power series into its first $N$ terms and the terms from $N$ to $\\infty$. And show separately that the two pieces are holomorphic. \n\n**Theorem.** The power series $f(z) = \\sum_{n=0}^\\infty a_nz^n$ defines a holomorphic function in its disc of convergence. the derivative of $f$ is also a power series obtained by differentiating term by term the series for $f$, i.e., \n$$\nf'(z) = \\sum_{n=0}^\\infty na_nz^{n-1}. \n$$Moreover, $f'$ has the same radius of convergence as $f$. \n\n*Proof.* The assertion about the radius of convergence of $f'$ follows from Hadamard's formula. Indeed, $\\lim\\_{n \\to \\infty} n^{1/n} = 1$, and therefore \n$$\n\\limsup |a_n|^{1/n} = \\limsup |na_n|^{1/n}\n$$so that $\\sum a_nz^n$ and $\\sum na_n z^n$ have the same radius of convergence, hence $\\sum a_nz^n$ and $\\sum na_nz^{n-1}$ do as well.\n\nTo prove the first assertion, we must show that the series \n$$\ng(z) = \\sum\\_{n=0}^\\infty na_nz^{n-1}\n$$gives the derivative of $f$. For that, let $R$ denote the radius of convergence of $f$, and suppose $|z\\_0| \u003c r \u003c R$. Write \n$$\nf(z) = S_N(z) + E_N(z)\n$$where\n$$\nS_N(z) = \\sum\\_{n=0}^N a_nz^n \\text{ and } E_N(z) = \\sum\\_{n = N+1}^\\infty a_nz^n. \n$$Then, if $h$ is chosen so that $|z_0 +h| \u003c r$ we have \n\n$$\n\\begin{align*}\n\\frac{f(z\\_0 + h) - f(z\\_0)}{h} - g(z\\_0) = \\left(\\frac{S\\_N(z\\_0 + h) - S\\_N(z\\_0)}{h} - S\\_N\\'(z\\_0) \\right) +\u0026 \\left(S\\_N\\'(z\\_0) - g(z\\_0) \\right)\\\\\n+\u0026 \\left(\\frac{E\\_N(z\\_0 + h) - E\\_N(z\\_0)}{h}. \\right)\n\\end{align*}\n$$Since $a^n - b^n = (a-b)(a^{n-1} + a^{n-2}b + \\cdots + ab^{n-2} + b^{n-1})$, we see that \n$$\n\\left\\vert \\frac{E\\_N(z\\_0 + h) = E\\_N(z\\_0)}{h} \\right\\vert \\leq \\sum\\_{n=N+1}^\\infty |a_n| \\left \\vert \\frac{(z\\_0 + h)^n - z\\_0^n}{h} \\right\\vert \\leq \\sum\\_{n = N+1}^\\infty |a_n|nr^{n-1}.\n$$where we have used the fact that $|z\\_0 + h| \u003c r$ and $|z\\_0| \u003c r$. The expression on the right is the tailend of a convergent series, since $g$ converges absolutely on $|z| \u003c R$. Therefore, given $\\epsilon \u003e 0$ we can fine $N\\_1$ o that $N \u003e N\\_1$ implies \n$$\n\\left \\vert \\frac{E\\_N(z\\_0 + h) - E\\_N(z\\_0)}{h} \\right\\vert \u003c \\epsilon. \n$$Also, since $\\lim\\_{N \\to \\infty} S\\_N'(z\\_0) = g(z\\_0)$, we can find $N\\_2$ so that $N \u003e N\\_2$ implies \n$$\n\\left \\vert S\\_N'(z\\_0) - g(z\\_0) \\right\\vert \u003c \\epsilon.\n$$If we fix $N$ so that both $N \u003e N\\_1$ and $N \u003e N\\_2$ hold, then we can find $\\delta \u003e0$ so that $|h| \u003c \\delta$ implies \n$$\n\\left \\vert \\frac{S\\_N(z\\_0 + h) - S\\_N(z\\_0)}{h} \u003c \\epsilon \\right\\vert\n$$because the derivative of a polynomial is obtained by differentiating term-by-term. Therefore \n$$\n\\left \\vert \\frac{f(z\\_0 + h) - f(z\\_0)}{h} - g(z\\_0) \\right\\vert \u003c 3\\epsilon\n$$whenever $|h| \u003c \\delta$ and thus $f$ is holomorphic with derivative $g$. \n\nNow that we have this result, applying it successively we get the following. \n\n**Corollary.** A power series is infinitely complex differentiable in its disc of convergence, and higher derivatives are also power series obtained by termwise differentiation. \n\nBecause of this, we make the following definition.\n\n**Definition (Analytic Function).** A function $f$ defined on an open set $\\Omega$ is said to be *analytic* at a point $z\\_0 \\in \\Omega$ if there exists a power series $\\sum a_n(z - z\\_0)^n$ centered at $z\\_0$, with positive radius of convergence so that \n$$\nf(z) = \\sum\\_{n=0}^\\infty a\\_n(z - z\\_0)^n\n$$for all $z$ in a neighborhood of $z\\_0$. ","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["complex-analysis"]},"/notes/Representation-Theory/Basic-Definitions":{"title":"Basic Definitions","content":"$\\R, \\Z, \\N, \\Q, \\Rn, \\E, \\C, \\MaxSpec, \\Gal$\n\n**Definition (Representation).** A *representation* of an associative algebra $A$ is a vector space $V$ equipped with a homomorphism $$\n\\rho \\colon V \\to \\text{End}(V).\n$$\nWe can also think of a representation as a left $A$-module where the multiplication is given by $$\na \\cdot v = \\rho(a)(v).\n$$\n**Definition (Subrepresentation).** A *subrepresentation* of a representation $V$ is a subspace $U \\subset V$ which is invariant under all operators $\\rho(a) \\in A$, i.e., for each $u \\in U, \\rho(a)(u) \\in U$ for all $a \\in A$. If $V\\_1$ and $V\\_2$ are two representations  of $A$, \n$$\n\\rho\\_1 \\colon A \\to \\text{End}(V\\_1), \\rho\\_2 \\colon A \\to \\text{End}(V\\_2)\n$$then $V_1 \\oplus V_2$ is a representation of $A$ given by \n$$\n\\begin{align*}\n\\rho \\colon \\to\u0026 \\text{End}(V\\_1 \\oplus V\\_2)\\\\\na \\mapsto \u0026 \\rho\\_1(a) \\oplus \\rho\\_2(a).\n\\end{align*}\n$$\nThen, as a module, this is $\\rho(a)(v\\_1, v\\_2) = (\\rho\\_1(a)(v\\_1), \\rho\\_2(a)(v\\_2))$. \n\n**Definition (Irreducible and Indecomposable).** A nonzero representation $V$ of $A$ is said to be *irreducible* if its only subrepresentations are $0$ and $V$. A representation is *indecomposable* if it cannot be written as a direct sum of two nonzero representations. \n\n**Remark.** Irreducible implies indecomposable but not vice versa. Indeed, suppose $A$ has a representation $V$ which is irreducible. Suppose that we could represent $V$ as $V\\_1 \\oplus V\\_2$, but then both of these are subrepresentations of $V$, a contradiction. However, a representation could be indecomposable but still have a subrepresentation as in the following example. \n\n**Example.** Consider the group $G = (\\R, +)$ and the representation \n\n$$\n\\begin{align*}\n\\rho \\to\u0026 GL(\\R^2)\\\\\nr \\mapsto \u0026 \\begin{pmatrix} 1 \u0026 r\\\\ 0 \u00261 \\end{pmatrix}\n\\end{align*}.\n$$\nSuppose that this representation is decomposable, i.e., there exists a $G$-invariant and linearly independent vectors $v = (v_1, v_2)$ and $w = (w_1, w_2) \\in \\R^2$. Then the following must be true \n$$\n\\begin{align*}\n\\rho(r)(v) =\u0026 \\lambda v\\\\\n\\rho(r)(w) =\u0026 \\gamma w\n\\end{align*}\n$$\nfor all $r \\in \\R$ and some $\\lambda, \\gamma \\in \\R$. This equation implies \n$$\n\\begin{align*}\nv\\_1 + v\\_2r =\u0026 \\lambda v\\_1\\\\\nv\\_2 =\u0026 \\lambda v\\_2.\n\\end{align*}\n$$\nSo $\\lambda = 1$ and $v\\_2r = 0$ for all $r \\in \\R$. Then $v\\_2 = 0$ and $\\R v= \\R e_1$. We reach the same conclusion with $w$, i.e., $\\R w = \\R e_1$. But this is a contradiction so $\\rho$ is indecomposable. Note that $\\rho$ is not irreducible because $\\lbrace(r, 0) \\mid r \\in \\R \\rbrace$ is invariant under this action. Indeed, \n$$\n\\rho(r)(r, 0) = \\begin{pmatrix}\n1 \u0026 r \\\\ 0 \u0026 1\n\\end{pmatrix} \\begin{pmatrix}\nr \\\\ 0\n\\end{pmatrix} = \\begin{pmatrix} r \\\\ 0 \\end{pmatrix}.\n$$\nSo $\\lbrace (r, 0) \\mid r \\in \\R \\rbrace$ is a subrepresentation. In some cases we could use the subrepresentation to create a decomposition of the representation. But in this case it would need to be of the form $\\lbrace (a,b) \\mid a, b \\in \\R, b \\neq 0 \\rbrace$ which cannot be $G$-invariant. Indeed, \n$$\n\\rho(r)(a, b) =  \\begin{pmatrix}\n1 \u0026 r \\\\ 0 \u0026 1\n\\end{pmatrix} \\begin{pmatrix}\na \\\\ b\n\\end{pmatrix} = \\begin{pmatrix} a + rb \\\\ b \\end{pmatrix}.\n$$\nFor $a + rb = a$ for all $r \\in \\R$ we would need to require that $b = 0$ which would not work for the decomposition. \n\n**Examples.** \n1. $V = 0$.\n2. $V = A$ and $\\rho \\colon A \\to \\text{End}(A)$ is defined as follows: $\\rho(a)$ is the operator of left multiplication by $a$, so that $\\rho(a)b = ab$. This is called the *regular* representation of $A$. \n3. $A = k$, a field. Then a representation of $A$ is a vector space over $\\R$. \n4. $A = k\\langle x\\_1, \\ldots , x\\_n \\rangle$. Then a representation of $A$ is just a vector space $V$ over $k$ with a collection of arbitrary linear operators $\\rho(x\\_1), \\ldots , \\rho(x\\_n) \\colon V \\to V$.  A representation over $A$ is $$ \\rho \\colon k\\langle x_1, \\ldots , x_n \\rangle \\to V$$where $V$ is a vector space over $k$. An element of $A$ looks like $\\sum_{i=1}^k a\\_i x\\_1^{m\\_{i1}} \\cdots x\\_n^{m\\_{in}}$ so $$ \\rho\\left(\\sum_{i=1}^k a\\_i x\\_1^{m\\_{i1}} \\cdots x\\_n^{m\\_{in}} \\right) = \\sum_{i=1}^k a\\_i \\rho(x\\_1)^{m\\_{i1}} \\cdots \\rho(x\\_n)^{m\\_{in}}.$$\n\n**Definition (Homomorphism).** Let $V_1, V_2$ be two representations of an algebra $A$. A *homomorphism* or *intertwining operator* $\\phi \\colon V\\_1 \\to V\\_2$ is a linear operator which commutes with the action of $A$, i.e., $\\phi(av) = a\\phi(v)$ for any $v \\in V\\_1$. This really means that if we have $\\rho\\_1 \\colon A \\to V\\_1$ and $\\rho\\_2 \\colon A \\to V\\_2$. Then for all $a \\in A$, $v \\in V\\_1$, \n$$\n\\phi(\\rho\\_1(a)(v)) = \\rho\\_2(a)(\\rho(v)).\n$$\nA homomorphism $\\phi$ is said to be an *isomorphism* if it is an isomorphism of vector spaces. The space of all homomorphisms of representations $V\\_1 \\to V\\_2$ is denoted by $\\text{Hom}(V\\_1, V\\_2)$. \n\n\n\n\n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["representation-theory"]},"/notes/Representation-Theory/Representation-Theory":{"title":"Representation Theory","content":"Table of Contents\n- [Basic Definitions](notes/Representation%20Theory/Basic%20Definitions.md) \n- [Schur's Lemma](notes/Representation%20Theory/Schur's%20Lemma.md)\n\n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["representation-theory"]},"/notes/Representation-Theory/Schurs-Lemma":{"title":"Schur's Lemma","content":"Schur's lemma is one of the foundational results in representation theory and very simple to prove. It helps us classify homomorphisms of irreducible representations. \n\n**Theorem (Schur's Lemma).** Let $V\\_1, V\\_2$ be [representations](notes/Representation%20Theory/Basic%20Definitions.md) of an algebra $A$ over any field $F$, not necessarily algebraically closed. Let $\\phi \\colon V\\_1 \\to V\\_2$ be a nonzero [homomorphism](notes/Representation%20Theory/Basic%20Definitions.md) of representations. Then \n1. If $V\\_1$ is irreducible, $\\phi$ is injective. \n2. If $V\\_2$ is irreducible, $\\phi$ is surjective. \nThus if both $V_1$ and $V_2$ are irreducible, $\\phi$ is an isomorphism.\n\nThis remarkable theorem tells us that any two irreducible representations of an algebra are isomorphic. \n\n*Proof.* \n1. Let $K$ be the kernel of $\\phi$. Then $K$ is a subrepresentation of $V\\_1$. Indeed, $\\ker \\phi = \\lbrace v \\in V\\_1 \\mid \\phi(v) = 0 \\rbrace$. Let $a \\in A$ and $v \\in \\ker \\phi$, then $$ \\phi(\\rho\\_1(a)(v)) = \\rho\\_2(a)(\\phi(v)) = \\rho\\_2(a)(0) = 0$$ so $\\rho\\_1(v)(a) \\in \\ker \\phi$ and thus it is invariant so it is indeed a subrepresentation. But since $\\phi \\neq 0$ this subrepresentation cannot be $V_1$. So by irreducibility of $V\\_1$ we must have $K = 0$. \n2. The Image $I$ of $\\phi$ is a subrepresentation of $V\\_2$. Indeed, let $I = \\lbrace v \\in V\\_2 \\mid \\text{ there exists } v\\_1 \\in V\\_1 \\text{ such that } \\phi(v\\_1) = v\\_2 \\rbrace$.   Then for all $a \\in A$, if $\\phi(v\\_1) = v\\_2$ we have $$\\rho\\_2(a)(v_2) = \\rho\\_2(a)(\\phi(v\\_1)) = \\phi(\\rho\\_1(a)(v\\_1))\\in I.$$ So $I$ is in fact a subrepresentation. Since $\\phi \\neq 0$, this subrepresentation cannot be 0, so by irreducibility of $V\\_2$ we must have $I = V\\_2$. $\\blacksquare$ \n\nIf our field is algebraically closed and our representations are finite dimensional there is a much stronger statement we can make. \n\n**Theorem (Schur's Lemma for Algebraically Closed Fields).** Let $V$ be a finite dimensional representation of an algebra $A$ over an algebraically closed field $k$, and let $\\phi \\colon V \\to V$ be an intertwining operator. Then $\\phi = \\lambda \\cdot I$ for some $\\lambda \\in k$. \n\n*Proof.* Let $\\lambda$ be an eigenvalue of $\\phi$. It exists since $k$ is algebraically closed. Then the operator $\\phi - \\lambda I$ is an intertwining operator $V \\to V$ which is not an isomorphism, since its determinant is zero. Thus by the version of Schur's lemma above, this operator is zero, and $\\phi = \\lambda I$. \n\nThis is very powerful, but it only works over an algebraically closed field since we must be able to find an eigenvalue. \n\n**Example.** Take $V = A = \\mathbb C$ as an $\\R$-algebra. Then consider the representation \n$$\n\\begin{align*}\n\\rho \\colon \\mathbb C \\to \u0026\\text{End}(\\mathbb C)\\\\\n\\rho(a) \\mapsto \u0026 \\begin{pmatrix}\n0 \u0026 -1 \\\\ 1 \u0026 0\n\\end{pmatrix}\n\\end{align*}\n$$\nThis is a rotation by 90 degrees so there are no invariant subspaces. Thus it is irreducible. Now let \n$$\n\\begin{align*}\n\\rho \\colon \\mathbb C \\to\u0026 \\mathbb C\\\\\n\\begin{pmatrix}x \\\\ y \\end{pmatrix} \\mapsto \u0026 \\begin{pmatrix}\n0 \u0026 -1 \\\\ 1 \u0026 0\n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\end{align*}\n$$Then the eigenvalues of $\\phi$ are the roots of $\\lambda^2 + 1= 0$ which are $\\pm i \\notin \\R$. So we cannot write $\\phi = \\lambda I$ for some $\\lambda \\in \\R$. \n\nFrom this we get a nice description of the irreducible representations of commutative algebras. \n\n**Corollary.** Let $A$ be a commutative algebra. Then every irreducible finite dimensional representation $V$ of $A$ is 1-dimensional. \n\n*Proof.* Let $V$ be irreducible. For any element $a \\in A$, the operator $\\rho(a) \\colon V \\to V$ is an intertwining operator. Indeed, \n$$\n\\rho(a)(\\rho(b)(v)) = \\rho(a)\\rho(b)(v) = \\rho(ab)(v) = \\rho(ba)(v) = \\rho(b)(\\rho(a)(v)).\n$$Thus by Schur's lemma, $rho(a)$ is a scalar operator for any $a \\in A$. Hence, every subspace of $V$ is a subrepresentation. But $V$ is irreducible, so the only subspaces are 0 and $V$. Thus $\\dim V = 1$. \n\n\nIt turns out that there is an infinite dimensional version of this as well.\n\n**Theorem (Infinite Dimensional Schur's Lemma).** Let $A$ be an algebra over $\\mathbb C$ and let $V$ be an irreducible representation of $A$ with at most a countable basis. Then any homomorphism of representations $\\phi \\colon V \\to V$ is a scalar operator.\n\n*Proof.* By the version of Schur's lemma above $D = \\text{End}_A(V)$, the set of homomorphisms, is a division algebra. Indeed, since $V$ is irreducible, any  homomorphism $\\phi \\colon V \\to V$ is an isomorphism and thus has an inverse. Now we will look at the dimension of $D$ as a vector space. Note that since $D$ is a division ring, $V$ is a free $D$-module, and thus $\\dim V \\geq \\dim D$, so $\\dim D$ is at most countable. \n\nNext, suppose $\\phi$ is not a scalar operator and consider $\\mathbb C(\\phi) \\subset D$. Since $\\phi$ is not a scalar and $\\mathbb C$ is algebraically closed, it must be transcendental. Note that the set of elements \n$$\n\\frac{1}{\\phi - \\lambda}\n$$\nfor $\\lambda \\in \\mathbb C$ are linearly independent, so $\\mathbb C(\\phi)$ has uncountable dimension. This is a contradiction since $\\mathbb C(\\phi) \\subset D$, so $\\phi$ must be a scalar. \n\n","lastmodified":"2023-05-09T13:49:38.575078244Z","tags":["representation-theory"]}}